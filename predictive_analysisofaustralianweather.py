# -*- coding: utf-8 -*-
"""predictive_analysisOfAustralianWeather.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RgYWKb8d2_J_WJbSo-mnGWNvr-idcTsK

##**Introduction**- 
###My dataset is about the weather of Australia in different cities from year 2008 till 2017.

### **Part 1- (Reading CSV file to DataFrame and showing initial 5 rows)**
"""

import matplotlib 
import matplotlib.pyplot as plt 
import matplotlib.ticker as mticker
import numpy as np
import io
import pandas as pd
import seaborn as sns
from google.colab import files
import datetime as dt

uploaded = files.upload()
original_data_of_ausWeather = pd.read_csv(io.BytesIO(uploaded['weatherAUS.csv']))
original_data_of_ausWeather.head(5)

"""### **Part 2- Univariate Analysis**"""

original_data_of_ausWeather.shape

original_data_of_ausWeather.dtypes

"""a)
**Problem**
Descriptive Statistics (using describe() function) 

**Output**
Returns the statistical data like mean,count, min etc.
"""

original_data_of_ausWeather.describe()

# To describe the values of string column like Location
# Output- count, unique, top and frequency
original_data_of_ausWeather['Location'].describe()

"""b) **Problem**- Identify presence of missing values and duplicates for each column and row

**Output**- Return the non-duplicate columns

"""

# Presence of Missing values- In this, i will identify which column contains null values
#Output- It will scan all the columns and check if any column contains the null value, if a column has a null value then it will return True else False

original_data_of_ausWeather.isnull().any()

# It will return the list of columns which contain the null values
original_data_of_ausWeather.loc[:,original_data_of_ausWeather.isnull().any()].columns

# Check whether there is any duplicate columns and rows in a data set

# -Check dulpicate columns dynamically

def tractduplicates(original_data_of_ausWeather):
    duplicateColumns=set()
    for cols in range(0,original_data_of_ausWeather.shape[1]):
      selected_col=original_data_of_ausWeather.iloc[:,cols]
      for new_col in range(cols+1,original_data_of_ausWeather.shape[1]):
        other_col=original_data_of_ausWeather.iloc[:,new_col]
        if selected_col.equals(other_col):
          duplicateColumns.add(original_data_of_ausWeather.columns.values[new_col]) 
    return list(duplicateColumns) 

if tractduplicates(original_data_of_ausWeather)==[]:
  print('No duplicate columns found')
  new_data_set=original_data_of_ausWeather
else:
  new_data_set=original_data_of_ausWeather.drop(columns = tractduplicates(original_data_of_ausWeather))
  new_data_set

# -Check dulpicate rows in a dataset

original_data_of_ausWeather.duplicated(subset=None, keep='first')

# After identify duplicates, now in this step, i will drop all the duplicates if found

data_set_after_droping_duplicates = original_data_of_ausWeather.drop_duplicates(subset=None, keep='first', inplace=False)
data_set_after_droping_duplicates

# Compare the original data set length and new data set length

print('Original data set length')
len(original_data_of_ausWeather)

location_column=new_data_set['Location'].drop_duplicates()
list_of_uniqueLocations=location_column.to_numpy()
list_of_uniqueLocations

year_array = pd.to_datetime(new_data_set['Date'], errors='coerce')
year_array=year_array.dt.year.drop_duplicates().sort_values().to_numpy()
year_array

# This histogram will show the average maximum temperature of different locations on January in last 10 years.

def monthlyAvgMaxTemp(year):
  janRange = (data_set_after_droping_duplicates['Date'] >= '{0}-01-01'.format(year)) & (data_set_after_droping_duplicates['Date'] <= '{0}-01-31'.format(year))
  janValue=data_set_after_droping_duplicates.loc[janRange]
  jan=janValue['MaxTemp'].mean()
  return jan

array_locyear=[]
for loca in list_of_uniqueLocations:
  for year in year_array:
    data_set_after_droping_duplicates=new_data_set.query('Location==@loca')
    my_string = '{0}-{1}-{2}'.format(loca,monthlyAvgMaxTemp(year),year)
    array_locyear.append(my_string)
set_Data=data_set_after_droping_duplicates
for value in array_locyear:
  val=value.split('-')
  if val[1]=='nan':
    val=1
  else:
    set_Data = set_Data.append({'Location': val[0],'Avg':float(val[1]),'Year':val[2]}, ignore_index=True)
set_Data=set_Data.sort_values(by=['Location'])
location="MountGambier"
for locationWise in list_of_uniqueLocations:
  new_dataByyear=set_Data.query('Location==@locationWise')
  updated_set=new_dataByyear[['Avg','Year']].sort_values(by=['Year'])

  hist_dimensions = (11.7, 8.27)
  fig = plt.subplots(figsize=hist_dimensions)
  hist_plot=sns.histplot(data=updated_set, x="Avg");
  hist_plot.set(xlabel='Average maximum temperature of {0} in past 10 years for January'.format(locationWise), ylabel='Count');

print('New data set length')
len(data_set_after_droping_duplicates)

"""c) **Problem**- Boxplot or Histogram for numeric columns
  **Output**- 
"""

#In this bar plot, the average maximum temperature of a particular location will be displayed annually.

bar_data=new_data_set
bar_data['Date'] = pd.to_datetime(new_data_set['Date'])
bar_array=[]

def locationWiseAnnualAvgTemp(year,location):
  bar_data1=set_new.query('Location==@location')
  avg=bar_data1['MaxTemp'].mean()
  return avg
location='Albury'
for year in year_array:
  set_new=bar_data[bar_data['Date'].dt.year == year]
  d=locationWiseAnnualAvgTemp(year,location)
  bar_array.append(d)

index = year_array
df = pd.DataFrame({'Average Maximum Temperature of {0}'.format(location): bar_array}, index=index)
ax = df.plot.bar(rot=0)

#Plotting line graph for maximum Temperature changes of july 2010 in particular location
from dateutil import rrule
from datetime import datetime
location='Albury'
def datelist():
  datesArray=[]
  a = '20100701'
  b = '20100731'
  for dt in rrule.rrule(rrule.DAILY,
                        dtstart=datetime.strptime(a, '%Y%m%d'),
                        until=datetime.strptime(b, '%Y%m%d')):
    datesArray.append(dt.strftime('%Y-%m-%d'))
  return datesArray
 
def plotLineByLocation(location):
  tempDate=new_data_set.query('Location==@location')
  mask = (tempDate['Date'] >= '2010-07-01') & (tempDate['Date'] <= '2010-07-31')
  tempDate=tempDate.loc[mask]
  return tempDate

x = np.array(datelist())
y = plotLineByLocation(location)['MaxTemp']
# plotting 
plt.rcParams["figure.figsize"] = (20,10)
plt.title("Change in maximum temperature in JULY 2010 of {0}".format(location))  
plt.xlabel("Month of July")  
plt.ylabel("Maximum temperature")  
#plt.figure(figsize=(20,10))
plt.plot(x, y, color ="red")  
plt.xticks(rotation=90)
plt.show()

"""###**Part 3- Multivariant Analysis**

**Scatter Plot- To compare numeric columns**
"""

# Average temperature and humidity over last 10 years of particular location

import matplotlib.cm as cm
import matplotlib.colors as colors
location="Albury"
data_for_scattered_plot=new_data_set[['Location','Date','Temp9am','Temp3pm','Humidity9am','Humidity3pm']]
data_for_scattered_plot=data_for_scattered_plot.query('Location==@location')
yearArray=[]
tempAvg=[]
humAvg=[]
#print(data_for_scattered_plot)
def scatterCalculations(year):
  temp_frame['avg_temp']=temp_frame[['Temp9am','Temp3pm']].mean(axis=1)
  temp_frame['avg_humidity']=temp_frame[['Humidity9am','Humidity3pm']].mean(axis=1)
  #print(temp_frame['avg_temp'])
  yearArray.append(year)
  tempAvg.append(temp_frame['avg_temp'].mean())
  humAvg.append(temp_frame['avg_humidity'].mean())
  #print(tempAvg)
for year in year_array:
  temp_frame=data_for_scattered_plot[data_for_scattered_plot['Date'].dt.year == year]
  #print(temp_frame)
  scatterCalculations(year)
#print(tempAvg)
fig, ax = plt.subplots()
data={'Year': yearArray, 'AverageTemperature': tempAvg,'AverageHumidity':humAvg}
df = pd.DataFrame(data)
colormap = cm.viridis
colorlist = [colors.rgb2hex(colormap(i)) for i in np.linspace(0, 0.9, len(df['Year']))]

for i,c in enumerate(colorlist):

    x = df['AverageTemperature'][i]
    y = df['AverageHumidity'][i]
    l = df['Year'][i]

    ax.scatter(x, y, label=l, s=100, linewidth=0.1, c=c)

ax.legend(loc='upper left', bbox_to_anchor=(0.75, 1.05),
          ncol=3, fancybox=True, shadow=True)
ax.set(title = f'{location} over last 10 years',
       xlabel = "Average Temperature",
       ylabel = "Average Humidity")
plt.show();

"""**Count Plot - Comparing categorical data**"""

#It is showing the number of times, the strongest wind gust has taken particular direction in 24 hour over last 10 years of particular location

import seaborn as sns
location="Albury"
data_for_count_plot=new_data_set[['Location','Date','WindGustDir']]
data_for_count_plot=data_for_count_plot.query('Location==@location')
data_for_count_plot
WindGustDir=['N','NNE','NE','ENE','E','ESE','SE','SSE','S','SSW','SW','WSW','W','WNW','NW','NNW']
data_for_count_plot['WindGustDir'].value_counts()
a4_dims = (11.7, 8.27)
fig, ax1 = plt.subplots(figsize=a4_dims)
ax=sns.countplot(x ='WindGustDir', data = data_for_count_plot,ax=ax1)
ax.set(xlabel='Direction of the strongest wind gust in the 24 hours to midnight in last 10 years', ylabel='Count')
# Show the plot
plt.title(f'Location- {location}')
plt.show()

"""### **Part 4 - After performing data exploration on country "Albury", now lets predict the chances of rainfall of tomorrow.**

**Step 1:** Selecting the range of city "Albury" data of year 2017
"""

filteredData=new_data_set[new_data_set['Location']=='Albury']

filteredData['Date'] = pd.to_datetime(filteredData['Date'])
alburyData = filteredData[(filteredData['Date'] >= '2017-02-25') & (filteredData['Date'] <= '2017-06-25')]
alburyData

"""**Step 2:** Check the nulls in data and replacing them."""

alburyData.isnull().sum()
alburyData2017=alburyData.drop(['Cloud3pm','Cloud9am','Date','Location','Evaporation','Sunshine'],axis=1)
alburyData2017.dropna(inplace=True)
alburyData2017

alburyData2017.isnull().sum()

"""**Step 3:** Converting categorical data into numeric data"""

RainTomorrow=pd.get_dummies(alburyData2017['RainTomorrow'],drop_first=True)
RainTomorrow.rename(columns={"Yes": "Rain_Tomorrow"},inplace=True)

RainToday=pd.get_dummies(alburyData2017['RainToday'],drop_first=True)
windGustDir=pd.get_dummies(alburyData2017['WindGustDir'],drop_first=True)

"""**Step 4:** Concatenate all the columns in main dataframe"""

alburyData2017_dummies=pd.concat([windGustDir,RainToday,RainTomorrow],axis=1)

alburyData2017Area=alburyData2017.drop(['WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow'],axis=1)

alburyData2017_final=pd.concat([alburyData2017_dummies,alburyData2017Area],axis=1)
alburyData2017_final=alburyData2017_final.dropna()
alburyData2017_final

"""###**Part 5 - Building Logistic Regression Model**

#### **Features Matrix and Target Vector**

**Target Vector(y)- Rain_tomorrow**


**Feature matrix(x)- Other columns except "Rain_tomorrow" in dataset "alburyData2017_final"**
"""

x=alburyData2017_final.drop(['Rain_Tomorrow'],axis=1)
y=alburyData2017_final['Rain_Tomorrow']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

from sklearn.linear_model import LogisticRegression
logModel=LogisticRegression()
logModel.fit(x_train,y_train)

predict=logModel.predict(x_test)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,predict)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,predict)

"""**The score appears to be 88% or 0.8.**

###**Conclusion-**

###**Therefore, the final score is above 0.5, which is considered as a very good score. Hence, the logistic model fits very well to make accurate rainfall prediction.**
"""